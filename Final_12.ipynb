{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ28Gau2kktB"
      },
      "source": [
        "# <center> **Text Mining Project: Final Solution.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZti3a5nkktL"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BCbNRHykktU"
      },
      "source": [
        "Group Number: 12 \n",
        "- Omar Jarir [m20201378@novaims.unl.pt]  \n",
        "- Chung-Ting Huang [m20210437@novaims.unl.pt] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KZaT7TLkktX"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND4Ti2RikktY"
      },
      "source": [
        "In our experiment, we found MLP using TF-IDF feature has the best result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvuVgQJImqIQ"
      },
      "source": [
        "# **1. Data import**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZR-GV9fmryV"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ScSqCsBZkkta"
      },
      "outputs": [],
      "source": [
        "import requests as rq\n",
        "from io import BytesIO\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import f_classif, SelectKBest\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "famfY4U1lOur",
        "outputId": "b71cdd85-d5b2-4923-8e46-988552036757"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tPqtNHjiqUTL"
      },
      "outputs": [],
      "source": [
        "random_state = 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eZXlqU_ck1zv"
      },
      "outputs": [],
      "source": [
        "url_train = \"https://raw.githubusercontent.com/omarja12/Text_Mining/main/train.csv\"\n",
        "url_test = \"https://raw.githubusercontent.com/omarja12/Text_Mining/main/test.csv\"\n",
        "data_train = rq.get(url_train).content\n",
        "data_test = rq.get(url_test).content\n",
        "ds_train = pd.read_csv(BytesIO(data_train))\n",
        "ds_test = pd.read_csv(BytesIO(data_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DWlLZVaFkktm"
      },
      "outputs": [],
      "source": [
        "ds_train.drop(columns=['Id'], inplace=True)\n",
        "ds_test.drop(columns=['Id'], inplace=True)\n",
        "ds_train[['Headline3', 'Headline11', 'Headline23']] = \\\n",
        "          ds_train[['Headline3', 'Headline11', 'Headline23']]. fillna(\" \")\n",
        "y_train = np.array(ds_train['Closing Status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rYZkOyn8nMpM"
      },
      "outputs": [],
      "source": [
        "stop = set(stopwords.words('english')).union(('U.S.', 'say', 'Say', 'says', 'year', 'new', 'New'))\n",
        "# Alternatively we can use SnowballStemmer\n",
        "stemmer = PorterStemmer()\n",
        "lemma = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xZtqLVvnk4t"
      },
      "source": [
        "# **2. Text cleaning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jicl9O-2nqGD"
      },
      "source": [
        "___ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kcmEUxJZkkto"
      },
      "outputs": [],
      "source": [
        "def clean(text_list, lemmatize = True, stemming = False):\n",
        "    '''\n",
        "    Clean the corpus by:\n",
        "    1. Change all words to lower case.\n",
        "    2. Remove numbers, punctuation, tags, hashtags, links, abbreviations and white spaces.\n",
        "    3. Remove stop words.\n",
        "    4. Lemmatize.\n",
        "    5. Stemming.\n",
        "    '''\n",
        "    updates = []\n",
        "    \n",
        "    for j in tqdm(text_list):\n",
        "        \n",
        "        text = j\n",
        "        \n",
        "        #LOWERCASE TEXT\n",
        "        text = text.lower()\n",
        "        \n",
        "        #REMOVE NUMERICAL DATA and PUNCTUATION\n",
        "        text = re.sub(\"[^a-zA-Z]\",\" \", text )\n",
        "\n",
        "        # TRANSFORM WORDS.\n",
        "        text = re.sub(r\"n't\", \" not \", text)\n",
        "        text = re.sub(r\"\\'s\", \" \", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "        text = re.sub(r\"\\'re\", \" are \", text)\n",
        "        text = re.sub(r\"\\'d\", \" would \", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "\n",
        "        # REMOVING HASHTAGS.\n",
        "        text = re.sub(\"@[A-Za-z0-9_]+\",\" \", text)\n",
        "        text = re.sub(\"#[A-Za-z0-9_]+\",\" \", text)\n",
        "          \n",
        "        # REMOVING NUMBERS FROM TEXT\n",
        "        text = \" \".join([word for word in text.split() if not word.isdigit()])\n",
        "                \n",
        "        # REMOVE TAGS\n",
        "        text = BeautifulSoup(text).get_text(separator=' ')\n",
        "        \n",
        "        #REMOVE STOP WORDS\n",
        "        text = \" \".join([word for word in text.split() if word not in stop])\n",
        "        \n",
        "        if lemmatize == True:\n",
        "            text = \" \".join([lemma.lemmatize(word, pos='n') for word in text.split()])\n",
        "        \n",
        "        if stemming == False:\n",
        "            text = \" \".join([stemmer.stem(word) for word in text.split()])     \n",
        "            \n",
        "         # REMOVING SINGLE CHARACTER WORDS.\n",
        "        text = re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
        "        \n",
        "        # Removing spaces\n",
        "        text = \" \".join(text.split())\n",
        "        \n",
        "        if len(text)== 0:\n",
        "            text = \" \"\n",
        "            \n",
        "        updates.append(text)    \n",
        "        \n",
        "    return updates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzPtbMkckktu"
      },
      "source": [
        "**Corpus cleaning:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEG1RKG-kktx",
        "outputId": "68add178-40f2-4408-f0e2-5bb8a034bd72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1690/1690 [00:04<00:00, 375.79it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 664.83it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 690.19it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 660.42it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 567.82it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 474.12it/s]\n",
            "100%|██████████| 1690/1690 [00:04<00:00, 418.14it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 580.92it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 616.02it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 705.59it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 764.46it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 736.87it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 797.63it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 773.05it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 746.23it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 714.01it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 740.99it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 702.28it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 721.09it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 895.72it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 971.83it/s] \n",
            "100%|██████████| 299/299 [00:00<00:00, 1021.07it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 888.25it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 929.04it/s]\n",
            "100%|██████████| 1690/1690 [00:02<00:00, 833.08it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 957.48it/s] \n",
            "100%|██████████| 1690/1690 [00:01<00:00, 922.78it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 946.68it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 878.16it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 898.41it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 931.89it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 869.63it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 917.01it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 940.74it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 871.63it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 1000.50it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 885.28it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 929.10it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 947.25it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 895.72it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 944.66it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 974.50it/s] \n",
            "100%|██████████| 1690/1690 [00:01<00:00, 913.05it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 937.84it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 877.03it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 943.70it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 946.21it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 947.10it/s]\n",
            "100%|██████████| 1690/1690 [00:01<00:00, 916.51it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 949.75it/s]\n"
          ]
        }
      ],
      "source": [
        "ds_train_clean = pd.DataFrame(columns=ds_train.columns)\n",
        "ds_test_clean = pd.DataFrame(columns=ds_test.columns)\n",
        "Headlines = list(ds_test.columns[0:])\n",
        "for col in Headlines:\n",
        "    clean_columns = clean(ds_train[col], lemmatize = True, stemming = False) \n",
        "    ds_train_clean[col] = clean_columns\n",
        "    clean_columns = clean(ds_test[col], lemmatize = True, stemming = False) \n",
        "    ds_test_clean[col] = clean_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze9yXffnkktz"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo1ip57kkt0"
      },
      "source": [
        "**Removing Common Words That Are Both Positive and Negative**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pq49luIEkkt1"
      },
      "outputs": [],
      "source": [
        "def TopnGrams(corpus, top_k, n):\n",
        "    '''\n",
        "    Returns a dataframe of ngrams\n",
        "    '''\n",
        "    vec = CountVectorizer(ngram_range=(n, n), max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = []\n",
        "    for word, idx in vec.vocabulary_.items():\n",
        "        words_freq.append((word, sum_words[0, idx]))\n",
        "        \n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    top_df = pd.DataFrame(words_freq[:top_k])\n",
        "    top_df.columns = [\"Ngram\", \"Freq\"]\n",
        "    return top_df\n",
        "    \n",
        "def top_common_words(train_pos, train_neg):\n",
        "    pos_ngrams = TopnGrams(train_pos, n=1, top_k=100)\n",
        "    neg_ngrams = TopnGrams(train_neg, n=1, top_k=100)\n",
        "    return pd.merge(pos_ngrams, neg_ngrams, on='Ngram')\n",
        "\n",
        "def remove_common_words(text_list, top_words):\n",
        "    '''\n",
        "    Remove top words that appear in positive and negative corpus\n",
        "    '''\n",
        "    updates = []\n",
        "    for j in tqdm(text_list):\n",
        "        text = j\n",
        "        \n",
        "        #REMOVE COMMON WORDS.\n",
        "        text = \" \".join([word for word in text.split() if word not in top_words['Ngram'].to_list()])\n",
        "        \n",
        "        updates.append(text)    \n",
        "        \n",
        "    return updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7QTqrZkkkt2",
        "outputId": "37a5ef7d-a396-49cd-c977-610855d1286d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 299/299 [00:00<00:00, 484.09it/s]\n"
          ]
        }
      ],
      "source": [
        "ds_train_clean['HeadlinesTotal'] = \\\n",
        "                 ds_train_clean[Headlines].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)\n",
        "ds_test_clean['HeadlinesTotal'] = \\\n",
        "                 ds_test_clean[Headlines].apply(lambda row: \" \".join(row.values.astype(str)), axis=1)\n",
        "\n",
        "train_pos = ds_train_clean.loc[ds_train['Closing Status'] == 1, 'HeadlinesTotal']\n",
        "train_neg = ds_train_clean.loc[ds_train['Closing Status'] == 0, 'HeadlinesTotal']\n",
        "\n",
        "top_words = top_common_words(train_pos=train_pos, train_neg=train_neg)\n",
        "clean_columns = remove_common_words(ds_test_clean['HeadlinesTotal'], top_words)\n",
        "ds_test_clean['HeadlinesTotal'] = clean_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AEdB3m0pZaa"
      },
      "source": [
        "# **3. Feature Engineering:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrFdbkbakkt4"
      },
      "source": [
        "**Creating TF-IDF Features:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O--bKCjCkkt6"
      },
      "outputs": [],
      "source": [
        "# Vectorization parameters:\n",
        "\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 300\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "# One of 'word', 'char'.\n",
        "TOKEN_MODE = 'word'\n",
        "\n",
        "# Minimum and Maximum document/corpus frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 10\n",
        "MAX_DOCUMENT_FREQUENCY = 0.8\n",
        "MAX_FEATURES=30000\n",
        "\n",
        "def tfidf(train_texts, train_labels, test_texts):\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
        "            'strip_accents': 'unicode',\n",
        "            'decode_error': 'replace',\n",
        "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
        "            'max_df': MAX_DOCUMENT_FREQUENCY,  \n",
        "            'max_features': MAX_FEATURES,\n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    x_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize test texts.\n",
        "    x_test = vectorizer.transform(test_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "    selector.fit(x_train, train_labels)\n",
        "    x_train = selector.transform(x_train).astype('float64')\n",
        "    x_test = selector.transform(x_test).astype('float64')\n",
        "    return x_train.toarray(), x_test.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JLz1RJETkkt7"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(ds_train['Closing Status'])\n",
        "X_tfidf, X_tfidf_test = tfidf(ds_train_clean[\"HeadlinesTotal\"], y_train, ds_test_clean[\"HeadlinesTotal\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8D5OlG3kkt8"
      },
      "source": [
        "# **4. Training the model:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MLP was our best performing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BYdclKHtkkt9",
        "outputId": "0ea12791-d1b9-4c9e-8d15-bbeb4045be76"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>299 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     prediction\n",
              "0             1\n",
              "1             1\n",
              "2             1\n",
              "3             1\n",
              "4             1\n",
              "..          ...\n",
              "294           1\n",
              "295           1\n",
              "296           0\n",
              "297           1\n",
              "298           1\n",
              "\n",
              "[299 rows x 1 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp = MLPClassifier(solver='adam', hidden_layer_sizes=(80,60,40), \n",
        "                         early_stopping=True,\n",
        "                         activation='relu', random_state=random_state)\n",
        "mlp.fit(X_tfidf, y_train)\n",
        "pred = pd.DataFrame(index=ds_test.index, columns=['prediction'])\n",
        "pred['prediction'] = mlp.predict(X_tfidf_test)\n",
        "pred.to_csv('Predictions_12.csv', sep=\",\")\n",
        "pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <center> **THE END.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final_12.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "906eca577c2ad557738f7e0b329f306984fd9e1bcf6cf4e2410c28cafd87502b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
